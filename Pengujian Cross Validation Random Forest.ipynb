{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fungsi untuk menghitung Entropy\n",
    "def entropy(y):\n",
    "    #Mehitung jumlah kemunculan kelas, hasil berupa array [a, b]\n",
    "    kemunculan_kelas = np.bincount(y)\n",
    "    #Menghitung probabilitas kemunculan kelas, hasil berupa array [a, b]\n",
    "    prob_kemunculan = kemunculan_kelas/len(y)\n",
    "    #Return rumus entropy, dengan kondisi px tidak kurang dari 0\n",
    "    return -np.sum([px * np.log2(px) for px in prob_kemunculan if px > 0])\n",
    "\n",
    "#Kelas pembantu untuk menyimpan informasi node\n",
    "class Node:\n",
    "    #Parameter dari Node yang ada pada sebuah Decision Tree yang nilainya perlu disimpan\n",
    "    #* digunakan untuk keyword only parameter\n",
    "    def __init__(self, fitur=None, threshold=None, node_kiri=None, node_kanan=None, *, nilai_leaf=None):\n",
    "        self.fitur = fitur\n",
    "        self.threshold = threshold\n",
    "        self.node_kiri = node_kiri\n",
    "        self.node_kanan = node_kanan\n",
    "        self.nilai_leaf = nilai_leaf\n",
    "    \n",
    "    #Fungsi pembantu untuk mengindikasikan bawah proses spilitting sudah sampai di node leaf\n",
    "    def leaf_node(self):\n",
    "        return self.nilai_leaf is not None\n",
    "    \n",
    "#Kelas Decision Tree\n",
    "class DecisionTree:\n",
    "    #Constructor dengan parameter untuk proses splitting\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, jumlah_fitur=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.jumlah_fitur = jumlah_fitur\n",
    "        self.root = None\n",
    "    \n",
    "    #Fungsi untuk melakukan training \n",
    "    def fit(self, X, y):\n",
    "        #Jumlah fitur = jumlah kolom dari x yang dipilih\n",
    "        #Dengan kondisi jika nanti jumlah_fitur tidak didefinisikan maka jumlah fitur yang dipilih adalah jumlah maksimal dari fitur yang dipilih\n",
    "        #Dan jumlah fitur yang dipilih tidak akan lebih besar dari jumlah_fitur yang ada\n",
    "        self.jumlah_fitur = X.shape[1] if not self.jumlah_fitur else min(self.jumlah_fitur, X.shape[1])\n",
    "        #Memulai pembentukan decision tree dari root\n",
    "        self.root = self.tree(X, y)\n",
    "    \n",
    "    #Fungsi untuk membentuk decision tree\n",
    "    def tree(self, X, y, depth=0):\n",
    "        #Mendapatkan nilai dari jumlah sample/baris dan nilai dari jumlah fitur/kolom yang ada pada data hasil pre-processing\n",
    "        n_samples, n_fitur = X.shape\n",
    "        #Mendapatkan nilai dari total kelas yang ada pada data hasil pre-processing\n",
    "        n_kelas = len(np.unique(y))\n",
    "        \n",
    "        #Stopping criteria untuk proses splitting\n",
    "        #Jika kondisi terpenuhi\n",
    "        if (depth >= self.max_depth\n",
    "                or n_kelas == 1\n",
    "                or n_samples < self.min_samples_split):\n",
    "            #Maka proses splitting sudah sampai di leaf , dan nilai pada leaf bisa didapatkan menggunakan fungsi nilai_kelas_terbanyak\n",
    "            nilai_leaf_node = self.nilai_kelas_terbanyak(y)\n",
    "            #Menyimpan nilai leaf pada leaf_node\n",
    "            return Node(nilai_leaf=nilai_leaf_node)\n",
    "        \n",
    "       \n",
    "        #Memilih secara random fitur yang ada pada n_fitur dengan jumlah yang ada pada jumlah_fitur tanpa pengembalian\n",
    "        fitur_dipilih = np.random.choice(n_fitur, self.jumlah_fitur, replace=False)\n",
    "        \n",
    "        #Mencari fitur terbaik dan threshold terbaik dari fitur yang telah dipilih secara random\n",
    "        fitur_terbaik, threshold_terbaik = self.feature_extraction(X, y, fitur_dipilih)\n",
    "        \n",
    "        #Membuat decision node berdasarkan rule (attribut dan threshold terbaik) yang telah dihitung melalui beberapa proses\n",
    "        #Memilih semua sample dengan fitur dan threshold terbaik\n",
    "        split_kiri, split_kanan = self.split(X[:, fitur_terbaik], threshold_terbaik)\n",
    "        #Memilih x dan y yang ada pada sample kiri untuk dijadikan sebagai sub decision node ( sudah dengan fitur dan threshold terbaik)\n",
    "        node_kiri = self.tree(X[split_kiri, :], y[split_kiri], depth+1)\n",
    "        #Memilih x dan y yang ada pada sample kanan  untuk dijadikan sebagai sub decision node ( sudah dengan fitur dan threshold terbaik)\n",
    "        node_kanan = self.tree(X[split_kanan, :], y[split_kanan], depth+1)\n",
    "        #Menyimpan nilai node\n",
    "        return Node(fitur_terbaik, threshold_terbaik, node_kiri, node_kanan)\n",
    "        \n",
    "    #Fungsi untuk mendapatkan fitur dan threshold terbaik\n",
    "    def feature_extraction(self, X, y, fitur_dipilih):\n",
    "        best_gain = -1\n",
    "        split_fitur, split_threshold = None, None\n",
    "        \n",
    "        for fitur_index in fitur_dipilih:\n",
    "            #Memilih kolom dari fitur yang dipilih\n",
    "            kolom_X = X[:, fitur_index]\n",
    "            #Mendapatkan nilai fitur yang dipilih/threshold secara unik\n",
    "            thresholds = np.unique(kolom_X)\n",
    "            for threshold in thresholds:\n",
    "                #Melakukan perhitungan entropy dan information gain untuk mendapatkan best threshold pada fungsi information_gain\n",
    "                gain = self.information_gain(y, kolom_X, threshold)\n",
    "                \n",
    "                #Kondisi untuk mendapatkan fitur dan threshold terbaik untuk dijadikan sebagai rule pada decision node\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_fitur = fitur_index\n",
    "                    split_threshold = threshold\n",
    "                    \n",
    "        return split_fitur, split_threshold\n",
    "        \n",
    "    #Fungsi untuk menghitung information gain\n",
    "    def information_gain(self, y, kolom_X, split_threshold):\n",
    "        #Menghitung parent entropy\n",
    "        parent_entropy = entropy(y)\n",
    "        \n",
    "        #Melakukan splitting dengan fungsi split\n",
    "        split_kiri, split_kanan = self.split(kolom_X, split_threshold)\n",
    "        \n",
    "        if len(split_kiri) == 0 or len(split_kanan) == 0:\n",
    "            return 0\n",
    "        \n",
    "        #Menghitung child entropy\n",
    "        total_kelas = len(y)\n",
    "        total_kelas_kiri = len(split_kiri)\n",
    "        total_kelas_kanan = len(split_kanan)\n",
    "        entropy_split_kiri = entropy(y[split_kiri])\n",
    "        entropy_split_kanan = entropy(y[split_kanan])\n",
    "        child_entropy = (total_kelas_kiri / total_kelas) * entropy_split_kiri + (total_kelas_kanan / total_kelas) * entropy_split_kanan\n",
    "        \n",
    "        #Information Gain\n",
    "        ig = parent_entropy - child_entropy\n",
    "        return ig\n",
    "        \n",
    "    #Fungsi untuk splitting    \n",
    "    def split(self, kolom_X, split_threshold):\n",
    "        split_kiri = np.argwhere(kolom_X <= split_threshold).flatten()\n",
    "        split_kanan = np.argwhere(kolom_X > split_threshold).flatten()\n",
    "        return split_kiri, split_kanan\n",
    "        \n",
    "        \n",
    "    #Fungsi untuk mendapatkan nilai kelas terbanyak pada leaf node\n",
    "    def nilai_kelas_terbanyak(self, y):\n",
    "        #Menghitung jumlah kemunculan kelas pada leaf node\n",
    "        counter = Counter(y)\n",
    "        #Mendapatkan nilai kemunculan kelas terbanyak\n",
    "        kemunculan_kelas_terbanyak = counter.most_common(1)[0][0]\n",
    "        #Menyimpan nilai tersebut untuk di pass\n",
    "        return kemunculan_kelas_terbanyak\n",
    "    \n",
    "    #Fungsi untuk melakukan prediksi\n",
    "    def predict(self, X):\n",
    "        #Bergerak dari awal root sampai ke leaf node\n",
    "        return np.array([self.pergerakan_tree(x, self.root) for x in X])\n",
    "    \n",
    "    def pergerakan_tree(self, x, node):\n",
    "        if node.leaf_node():\n",
    "            return node.nilai_leaf\n",
    "        \n",
    "        if x[node.fitur] <= node.threshold:\n",
    "            return self.pergerakan_tree(x, node.node_kiri)\n",
    "        return self.pergerakan_tree(x, node.node_kanan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapped_dataset(X, y):\n",
    "    sampel_data = X.shape[0]\n",
    "    #Memilih secara random dari sampel data dengan pengembalian untuk dijadikan sebagai subset data yang berfungsi untuk training decision tree\n",
    "    bootstrap_data = np.random.choice(sampel_data, size=sampel_data, replace=True)\n",
    "    return X[bootstrap_data], y[bootstrap_data]\n",
    "\n",
    "def voting(y):\n",
    "    counter = Counter(y)\n",
    "    #Mendapatkan nilai kemunculan kelas terbanyak\n",
    "    kemunculan_kelas_terbanyak = counter.most_common(1)[0][0]\n",
    "    #Menyimpan nilai tersebut untuk di pass\n",
    "    return kemunculan_kelas_terbanyak\n",
    "\n",
    "class RandomForest:\n",
    "    #\n",
    "    def __init__(self, jumlah_tree, min_samples_split=2, max_depth=100, jumlah_fitur=None):\n",
    "        self.jumlah_tree = jumlah_tree\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.jumlah_fitur = jumlah_fitur\n",
    "        #Untuk menyimpan informasi dari setiap decision tree yang telah dibuat\n",
    "        self.array_tree = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.array_tree = []\n",
    "        #Melakukan training terhadap setiap decision dari sejumlah n decision tree berdasarkan bootstrapped dataset\n",
    "        for single_tree in range(self.jumlah_tree):\n",
    "            tree = DecisionTree(min_samples_split=self.min_samples_split, max_depth=self.max_depth, jumlah_fitur=self.jumlah_fitur)\n",
    "            X_subset, y_subset = bootstrapped_dataset(X, y)\n",
    "            tree.fit(X_subset, y_subset)\n",
    "            #Menambahkan decision tree hasil training ke dalam array tree\n",
    "            self.array_tree.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        #\n",
    "        tree_prediksi = np.array([tree.predict(X) for tree in self.array_tree])\n",
    "        #\n",
    "        tree_prediksi = np.swapaxes(tree_prediksi, 0, 1)\n",
    "        #Melakukan voting dari n decision tree yang telah dibangun\n",
    "        prediksi_y = [voting(sekumpulan_tree) for sekumpulan_tree in tree_prediksi]\n",
    "        return np.array(prediksi_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-86342296351f>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = x.astype(np.float)\n",
      "<ipython-input-9-86342296351f>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y_new.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_excel('DataSaham.xlsx')\n",
    "data = data1.dropna()\n",
    "\n",
    "ArrayData = np.array(data)\n",
    "x = ArrayData[:2000,5:15]\n",
    "X = x.astype(np.float)\n",
    "Y_1D = ArrayData[:2000,15:16]\n",
    "y_new = Y_1D.ravel()\n",
    "y = y_new.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockingTimeSeriesSplit():\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        margin = 0\n",
    "        for i in range(self.n_splits):\n",
    "            start = (i * k_fold_size // 2)\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.80 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1\n",
      "TRAIN : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399] TEST : [400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417\n",
      " 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435\n",
      " 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453\n",
      " 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471\n",
      " 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489\n",
      " 490 491 492 493 494 495 496 497 498 499]\n",
      "Fold  2\n",
      "TRAIN : [250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267\n",
      " 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285\n",
      " 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303\n",
      " 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321\n",
      " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
      " 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
      " 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375\n",
      " 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393\n",
      " 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411\n",
      " 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429\n",
      " 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447\n",
      " 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465\n",
      " 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483\n",
      " 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501\n",
      " 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519\n",
      " 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537\n",
      " 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555\n",
      " 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573\n",
      " 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591\n",
      " 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609\n",
      " 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627\n",
      " 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645\n",
      " 646 647 648 649] TEST : [650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667\n",
      " 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685\n",
      " 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703\n",
      " 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721\n",
      " 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739\n",
      " 740 741 742 743 744 745 746 747 748 749]\n",
      "Fold  3\n",
      "TRAIN : [500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535\n",
      " 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553\n",
      " 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571\n",
      " 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589\n",
      " 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607\n",
      " 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625\n",
      " 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643\n",
      " 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661\n",
      " 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679\n",
      " 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697\n",
      " 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715\n",
      " 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733\n",
      " 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751\n",
      " 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769\n",
      " 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787\n",
      " 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805\n",
      " 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823\n",
      " 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841\n",
      " 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859\n",
      " 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877\n",
      " 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895\n",
      " 896 897 898 899] TEST : [900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917\n",
      " 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935\n",
      " 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953\n",
      " 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971\n",
      " 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989\n",
      " 990 991 992 993 994 995 996 997 998 999]\n",
      "Fold  4\n",
      "TRAIN : [ 750  751  752  753  754  755  756  757  758  759  760  761  762  763\n",
      "  764  765  766  767  768  769  770  771  772  773  774  775  776  777\n",
      "  778  779  780  781  782  783  784  785  786  787  788  789  790  791\n",
      "  792  793  794  795  796  797  798  799  800  801  802  803  804  805\n",
      "  806  807  808  809  810  811  812  813  814  815  816  817  818  819\n",
      "  820  821  822  823  824  825  826  827  828  829  830  831  832  833\n",
      "  834  835  836  837  838  839  840  841  842  843  844  845  846  847\n",
      "  848  849  850  851  852  853  854  855  856  857  858  859  860  861\n",
      "  862  863  864  865  866  867  868  869  870  871  872  873  874  875\n",
      "  876  877  878  879  880  881  882  883  884  885  886  887  888  889\n",
      "  890  891  892  893  894  895  896  897  898  899  900  901  902  903\n",
      "  904  905  906  907  908  909  910  911  912  913  914  915  916  917\n",
      "  918  919  920  921  922  923  924  925  926  927  928  929  930  931\n",
      "  932  933  934  935  936  937  938  939  940  941  942  943  944  945\n",
      "  946  947  948  949  950  951  952  953  954  955  956  957  958  959\n",
      "  960  961  962  963  964  965  966  967  968  969  970  971  972  973\n",
      "  974  975  976  977  978  979  980  981  982  983  984  985  986  987\n",
      "  988  989  990  991  992  993  994  995  996  997  998  999 1000 1001\n",
      " 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015\n",
      " 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029\n",
      " 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043\n",
      " 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057\n",
      " 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071\n",
      " 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085\n",
      " 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099\n",
      " 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113\n",
      " 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127\n",
      " 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141\n",
      " 1142 1143 1144 1145 1146 1147 1148 1149] TEST : [1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163\n",
      " 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177\n",
      " 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191\n",
      " 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205\n",
      " 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219\n",
      " 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233\n",
      " 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247\n",
      " 1248 1249]\n"
     ]
    }
   ],
   "source": [
    "btss = BlockingTimeSeriesSplit(n_splits=4)\n",
    "\n",
    "i=1\n",
    "array_y_test = []\n",
    "array_y_pred = []\n",
    "\n",
    "for train_index, test_index in btss.split(X, y):\n",
    "    print(\"Fold \", i)\n",
    "    print(\"TRAIN :\", train_index, \"TEST :\", test_index)\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    \n",
    "    array_y_pred.append(get_score(RandomForest(jumlah_tree=10, max_depth=10), X_train, X_test, y_train, y_test))\n",
    "    array_y_test.append(y_test)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.61\n",
      "Precision : 0.71875\n",
      "Recall : 0.6865671641791045\n",
      "F1 Score : 0.7022900763358778\n"
     ]
    }
   ],
   "source": [
    "rf_matrix = confusion_matrix(array_y_test[0], array_y_pred[0])\n",
    "#display(rf_matrix)\n",
    "\n",
    "true_negative = rf_matrix[0][0]\n",
    "false_negative = rf_matrix[1][0]\n",
    "true_positive = rf_matrix[1][1]\n",
    "false_positive = rf_matrix[0][1]\n",
    "\n",
    "accuracy = (true_negative + true_positive) / (true_negative + false_negative + true_positive + false_positive)\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "#specifity = true_negative / (true_negative + false_positive)\n",
    "F1_Score = 2 * (recall * precision) / (recall + precision)\n",
    "\n",
    "print('Accuracy :',format(float(accuracy)))\n",
    "print('Precision :',format(float(precision)))\n",
    "print('Recall :',format(float(recall)))\n",
    "#print('Specifity : ',format(float(specifity)))\n",
    "print('F1 Score :',format(float(F1_Score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
